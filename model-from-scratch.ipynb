{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mirabirhossain/model-from-scratch?scriptVersionId=144365389\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as f\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchsummary import summary\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:44:11.141068Z","iopub.execute_input":"2023-09-19T13:44:11.141465Z","iopub.status.idle":"2023-09-19T13:44:16.409305Z","shell.execute_reply.started":"2023-09-19T13:44:11.141432Z","shell.execute_reply":"2023-09-19T13:44:16.407585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the seed so that the outcome of the model is reproducable\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:03:30.645305Z","iopub.execute_input":"2023-09-19T13:03:30.646477Z","iopub.status.idle":"2023-09-19T13:03:30.653167Z","shell.execute_reply.started":"2023-09-19T13:03:30.646435Z","shell.execute_reply":"2023-09-19T13:03:30.651928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and Test data directory\ntrain_data_dir = '/kaggle/input/mangoleafbdomdena/MangoLeafBDomdena/Train'\ntest_data_dir = '/kaggle/input/mangoleafbdomdena/MangoLeafBDomdena/Test'","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:03:30.960366Z","iopub.execute_input":"2023-09-19T13:03:30.962643Z","iopub.status.idle":"2023-09-19T13:03:30.969342Z","shell.execute_reply.started":"2023-09-19T13:03:30.962574Z","shell.execute_reply":"2023-09-19T13:03:30.967822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the augmentation for train and test data\n# For train data we are randomly flipping the images horizontally or vertically 30% of the time\n# We resize the image to (224, 224) for both train and test data\n# We normalize the images to have a 0.5 mean and 0.5 std which is often used in literature\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(0.3),\n    transforms.RandomVerticalFlip(0.3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:03:31.305706Z","iopub.execute_input":"2023-09-19T13:03:31.306107Z","iopub.status.idle":"2023-09-19T13:03:31.314619Z","shell.execute_reply.started":"2023-09-19T13:03:31.306073Z","shell.execute_reply":"2023-09-19T13:03:31.313385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the PyTorch Dataset from train and test directory cause\n# PyTorch can work with PyTorch Datasets\ntrain_dataset = ImageFolder(train_data_dir, transform=train_transform)\ntest_dataset = ImageFolder(test_data_dir, transform=test_transform)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:03:31.555358Z","iopub.execute_input":"2023-09-19T13:03:31.555826Z","iopub.status.idle":"2023-09-19T13:03:31.604782Z","shell.execute_reply.started":"2023-09-19T13:03:31.555792Z","shell.execute_reply":"2023-09-19T13:03:31.603435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the length of the train and test dataset\nlen(train_dataset), len(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:03:31.871129Z","iopub.execute_input":"2023-09-19T13:03:31.87257Z","iopub.status.idle":"2023-09-19T13:03:31.881146Z","shell.execute_reply.started":"2023-09-19T13:03:31.872521Z","shell.execute_reply":"2023-09-19T13:03:31.87977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dividing the train data into train set and validation set\n# Creating train and validation indices\n# using \"stratify\" is making sure that every class gets equal portion of indices while spliting\n\ntrain_indices, valid_indices = train_test_split(range(len(train_dataset)), test_size=0.1,\n                                               stratify=train_dataset.targets,\n                                               random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:03:32.179157Z","iopub.execute_input":"2023-09-19T13:03:32.180593Z","iopub.status.idle":"2023-09-19T13:03:32.194388Z","shell.execute_reply.started":"2023-09-19T13:03:32.180537Z","shell.execute_reply":"2023-09-19T13:03:32.192626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the train and validation indices to randomly sample them for train and validation set\ntrain_sampler = data.SubsetRandomSampler(train_indices)\nvalid_sampler = data.SubsetRandomSampler(valid_indices)\n\n# Creating PyTorch DataLoaders to divide the dataset into batches to feed the model\n# Models built in PyTorch can only take data from DataLoaders\ntrainloader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler)\nvalidloader = DataLoader(train_dataset, batch_size=64, sampler=valid_sampler)\ntestloader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:03:32.499483Z","iopub.execute_input":"2023-09-19T13:03:32.500954Z","iopub.status.idle":"2023-09-19T13:03:32.510372Z","shell.execute_reply.started":"2023-09-19T13:03:32.5009Z","shell.execute_reply":"2023-09-19T13:03:32.507902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Designing the model\n# This is the final model after some training and parameter tuning\n\n# Depthwise Separable Convolution block\nclass DSCBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, pool='max'):\n        super().__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n        if pool == 'max':\n            self.pooling = nn.MaxPool2d(2, 2)\n        elif pool == 'avg':\n            self.pooling = nn.AvgPool2d(13, 13)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.pooling(x)\n        return x\n        \n# Full architecture using the Depthwise Separable Convolution blocks\nclass CNNarch(nn.Module):\n    def __init__(self, block):\n        super().__init__()\n        self.blocks = nn.Sequential(\n            nn.Conv2d(3, 32, 3, 1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2), # 111\n            block(32, 64), # 55\n            block(64, 128), # 27\n            block(128, 256), # 13\n            block(256, 256, 'avg'), \n        )\n        \n        self.linear = nn.Sequential(\n            nn.Linear(256, 48),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(48, 8)\n        )\n        \n    def forward(self, x):\n        x = self.blocks(x)\n        x = x.view(x.size(0), -1)\n        x = self.linear(x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:13:30.403647Z","iopub.execute_input":"2023-09-19T13:13:30.404055Z","iopub.status.idle":"2023-09-19T13:13:30.420491Z","shell.execute_reply.started":"2023-09-19T13:13:30.404021Z","shell.execute_reply":"2023-09-19T13:13:30.419087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a instance of model to use the torchsummary to see the summary of the model\nmodel = CNNarch(DSCBlock).to(device)\nm1 = model(torch.randn(1, 3, 224, 224).to(device))\nsummary(model, (3, 224, 224))","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:13:34.066112Z","iopub.execute_input":"2023-09-19T13:13:34.067345Z","iopub.status.idle":"2023-09-19T13:13:34.087928Z","shell.execute_reply.started":"2023-09-19T13:13:34.067299Z","shell.execute_reply":"2023-09-19T13:13:34.08675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From above, the model has 129,144 parameters, which is very lightweight and parameter size is only 0.49MB**","metadata":{}},{"cell_type":"code","source":"# Creating class weights because in the dataset there is class imbalance\n\nclass_weights = []\n\ntotal_samples = len(trainloader.dataset)\nnum_classes = len(trainloader.dataset.classes)\n\nfor class_idx in range(num_classes):\n    class_count = torch.sum(torch.tensor(trainloader.dataset.targets) == class_idx)\n    class_weight = total_samples / (num_classes * class_count)\n    class_weights.append(class_weight)\n\nclass_weights = torch.FloatTensor(class_weights).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:13:34.931867Z","iopub.execute_input":"2023-09-19T13:13:34.932853Z","iopub.status.idle":"2023-09-19T13:13:34.954651Z","shell.execute_reply.started":"2023-09-19T13:13:34.93281Z","shell.execute_reply":"2023-09-19T13:13:34.953262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can see the class weights in the output cell\nclass_weights","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:13:36.540497Z","iopub.execute_input":"2023-09-19T13:13:36.54295Z","iopub.status.idle":"2023-09-19T13:13:36.552713Z","shell.execute_reply.started":"2023-09-19T13:13:36.542899Z","shell.execute_reply":"2023-09-19T13:13:36.551423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the Loss function and the algorithm for model training\n# Cross Entropy Loss as loss function and Adam as optimizer\n\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.Adam(model.parameters(), lr=0.005)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:13:56.576717Z","iopub.execute_input":"2023-09-19T13:13:56.577112Z","iopub.status.idle":"2023-09-19T13:13:56.583852Z","shell.execute_reply.started":"2023-09-19T13:13:56.577081Z","shell.execute_reply":"2023-09-19T13:13:56.582565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model for 40 epochs with learning rate of 0.005\n\nnum_epochs = 40\n\ntrain_losses = []\nvalid_losses = []\n\ntrain_accuracies = []\nvalid_accuracies = []\n\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    \n    for inputs, targets in tqdm(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total_train += targets.size(0)\n        correct_train += (predicted == targets).sum().item()\n    \n    train_accuracy = 100 * correct_train / total_train\n    train_losses.append(train_loss / len(trainloader))\n    train_accuracies.append(train_accuracy)\n    \n    # Validation\n    model.eval()\n    valid_loss = 0.0\n    correct_valid = 0\n    total_valid = 0\n    \n    with torch.no_grad():\n        for inputs, targets in tqdm(validloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            valid_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_valid += targets.size(0)\n            correct_valid += (predicted == targets).sum().item()\n    \n    valid_accuracy = 100 * correct_valid / total_valid\n    valid_losses.append(valid_loss / len(validloader))\n    valid_accuracies.append(valid_accuracy)\n    \n    print(f'Epoch [{epoch + 1}/{num_epochs}]')\n    print(f'Training Loss: {train_losses[-1]:.4f} | Training Accuracy: {train_accuracy:.2f}%')\n    print(f'Validation Loss: {valid_losses[-1]:.4f} | Validation Accuracy: {valid_accuracy:.2f}%')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:13:57.61571Z","iopub.execute_input":"2023-09-19T13:13:57.618875Z","iopub.status.idle":"2023-09-19T13:27:17.225906Z","shell.execute_reply.started":"2023-09-19T13:13:57.618826Z","shell.execute_reply":"2023-09-19T13:27:17.224142Z"},"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training and validation losses\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(valid_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Losses')\n\n# Plot training and validation accuracies\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label='Training Accuracy')\nplt.plot(valid_accuracies, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend()\nplt.title('Training and Validation Accuracies')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:27:33.514958Z","iopub.execute_input":"2023-09-19T13:27:33.515455Z","iopub.status.idle":"2023-09-19T13:27:34.377071Z","shell.execute_reply.started":"2023-09-19T13:27:33.515417Z","shell.execute_reply":"2023-09-19T13:27:34.37597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the model performance with the testset\nmodel.eval()\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for inputs, targets in testloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        \n        y_true.extend(targets.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:27:43.727683Z","iopub.execute_input":"2023-09-19T13:27:43.729062Z","iopub.status.idle":"2023-09-19T13:27:46.051403Z","shell.execute_reply.started":"2023-09-19T13:27:43.729013Z","shell.execute_reply":"2023-09-19T13:27:46.050179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate and print the confusion matrix\nconfusion_mat = confusion_matrix(y_true, y_pred)\nprint(\"Confusion Matrix:\")\nprint(confusion_mat)\n\n# Calculate and print classification report\nreport = classification_report(y_true, y_pred, target_names=testloader.dataset.classes)\nprint(\"Classification Report:\")\nprint(report)\n\n# Calculate and print additional metrics\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average='weighted')\nrecall = recall_score(y_true, y_pred, average='weighted')\nf1 = f1_score(y_true, y_pred, average='weighted')\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:27:46.20149Z","iopub.execute_input":"2023-09-19T13:27:46.201932Z","iopub.status.idle":"2023-09-19T13:27:46.237184Z","shell.execute_reply.started":"2023-09-19T13:27:46.201894Z","shell.execute_reply":"2023-09-19T13:27:46.235772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see we get a very good accuracy and precision in test set**","metadata":{}},{"cell_type":"code","source":"# Saving the model and the weights\ntorch.save(model, 'full_model.pth')\ntorch.save(model.state_dict(), 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:29:12.954633Z","iopub.execute_input":"2023-09-19T13:29:12.955131Z","iopub.status.idle":"2023-09-19T13:29:12.988886Z","shell.execute_reply.started":"2023-09-19T13:29:12.955085Z","shell.execute_reply":"2023-09-19T13:29:12.987768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the model size\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef model_size_mb(model):\n    total_params = count_parameters(model)\n    model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n    model_size_mb = model_size_bytes / (1024 * 1024)  # Convert to megabytes (MB)\n    return model_size_mb\n\nsize_mb = model_size_mb(model)\n\nprint(f\"Model size: {size_mb:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:32:10.712647Z","iopub.execute_input":"2023-09-19T13:32:10.713058Z","iopub.status.idle":"2023-09-19T13:32:10.721997Z","shell.execute_reply.started":"2023-09-19T13:32:10.713024Z","shell.execute_reply":"2023-09-19T13:32:10.720652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see, we have a very light model of only 0.49 MB**","metadata":{}},{"cell_type":"code","source":"# Testing some random images and plot them to see their acutal label and predicted label\n\nmodel.eval()\n\ndef plot_random_test_samples(model, test_loader, num_samples=12):\n    # Get a batch of test data\n    data_iter = iter(test_loader)\n    images, labels = next(data_iter)\n    \n    # Move data to GPU\n    images = images.to(device)\n    labels = labels.to(device)\n\n    # Make predictions\n    with torch.no_grad():\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n    \n    # Plot random samples\n    num_rows = 3\n    num_cols = 4\n\n    plt.figure(figsize=(12, 9))\n\n    for i in range(num_samples):\n        index = np.random.randint(0, images.size(0))\n        image = images[index].cpu().numpy()\n        label = labels[index].item()\n        prediction = predicted[index].item()\n\n        plt.subplot(num_rows, num_cols, i + 1)\n        plt.imshow(np.transpose(image, (1, 2, 0)), cmap='gray')\n        plt.title(f\"True: {label}, Pred: {prediction}\")\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n\nplot_random_test_samples(model, testloader, num_samples=12)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:40:37.180784Z","iopub.execute_input":"2023-09-19T13:40:37.181297Z","iopub.status.idle":"2023-09-19T13:40:39.614539Z","shell.execute_reply.started":"2023-09-19T13:40:37.181228Z","shell.execute_reply":"2023-09-19T13:40:39.613422Z"},"trusted":true},"execution_count":null,"outputs":[]}]}